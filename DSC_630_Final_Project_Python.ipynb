{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DSC 630 - Final Project - Python Portion__\n",
    "\n",
    "__Brandon May__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Background__\n",
    "\n",
    "Fetal heart rate monitoring as an indicator of fetal well-being can be inaccurate as predictors of a poor neonatal outcome and come with significant healthcare and medicolegal costs.  It is the goal of this project to use a database of specific technical characteristics of fetal heart rate monitoring from the UCI machine learning database to develop a predictive model using an automated system to better identify worrisome decreases in fetal heart rate.\n",
    "\n",
    "The data that I will primarily use is from the University of California â€“ Irvine Machine Learning Repository.  The dataset can be found at the following web address: https://archive.ics.uci.edu/ml/datasets/Cardiotocography.\n",
    "\n",
    "According to the website, there were over 2000 fetal heart tracings (cardiotocograms) and interpreted by three expert obstetricians.  Many of the measurements include the technical measurements include heart rate accelerations, decelerations, max heart rate, minimum heart rates, heart rate baseline and finally the target variable is whether the tracing was normal, suspect, or pathologic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>LBE</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>MLTV</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>DR</th>\n",
       "      <th>Width</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>411.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>533.0</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>2059.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>2596.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>3049.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>2796.0</td>\n",
       "      <td>3415.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2126 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           b       e    LBE     LB   AC   FM   UC  ASTV  MSTV  ALTV  MLTV  \\\n",
       "1      240.0   357.0  120.0  120.0  0.0  0.0  0.0  73.0   0.5  43.0   2.4   \n",
       "2        5.0   632.0  132.0  132.0  4.0  0.0  4.0  17.0   2.1   0.0  10.4   \n",
       "3      177.0   779.0  133.0  133.0  2.0  0.0  5.0  16.0   2.1   0.0  13.4   \n",
       "4      411.0  1192.0  134.0  134.0  2.0  0.0  6.0  16.0   2.4   0.0  23.0   \n",
       "5      533.0  1147.0  132.0  132.0  4.0  0.0  5.0  16.0   2.4   0.0  19.9   \n",
       "...      ...     ...    ...    ...  ...  ...  ...   ...   ...   ...   ...   \n",
       "2122  2059.0  2867.0  140.0  140.0  0.0  0.0  6.0  79.0   0.2  25.0   7.2   \n",
       "2123  1576.0  2867.0  140.0  140.0  1.0  0.0  9.0  78.0   0.4  22.0   7.1   \n",
       "2124  1576.0  2596.0  140.0  140.0  1.0  0.0  7.0  79.0   0.4  20.0   6.1   \n",
       "2125  1576.0  3049.0  140.0  140.0  1.0  0.0  9.0  78.0   0.4  27.0   7.0   \n",
       "2126  2796.0  3415.0  142.0  142.0  1.0  1.0  5.0  74.0   0.4  36.0   5.0   \n",
       "\n",
       "       DL   DS   DP   DR  Width    Min    Max  Nmax  Nzeros   Mode   Mean  \\\n",
       "1     0.0  0.0  0.0  0.0   64.0   62.0  126.0   2.0     0.0  120.0  137.0   \n",
       "2     2.0  0.0  0.0  0.0  130.0   68.0  198.0   6.0     1.0  141.0  136.0   \n",
       "3     2.0  0.0  0.0  0.0  130.0   68.0  198.0   5.0     1.0  141.0  135.0   \n",
       "4     2.0  0.0  0.0  0.0  117.0   53.0  170.0  11.0     0.0  137.0  134.0   \n",
       "5     0.0  0.0  0.0  0.0  117.0   53.0  170.0   9.0     0.0  137.0  136.0   \n",
       "...   ...  ...  ...  ...    ...    ...    ...   ...     ...    ...    ...   \n",
       "2122  0.0  0.0  0.0  0.0   40.0  137.0  177.0   4.0     0.0  153.0  150.0   \n",
       "2123  0.0  0.0  0.0  0.0   66.0  103.0  169.0   6.0     0.0  152.0  148.0   \n",
       "2124  0.0  0.0  0.0  0.0   67.0  103.0  170.0   5.0     0.0  153.0  148.0   \n",
       "2125  0.0  0.0  0.0  0.0   66.0  103.0  169.0   6.0     0.0  152.0  147.0   \n",
       "2126  0.0  0.0  0.0  0.0   42.0  117.0  159.0   2.0     1.0  145.0  143.0   \n",
       "\n",
       "      Median  Variance  Tendency    A    B    C    D    E   AD   DE   LD   FS  \\\n",
       "1      121.0      73.0       1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2      140.0      12.0       0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "3      138.0      13.0       0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "4      137.0      13.0       1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "5      138.0      11.0       1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...      ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2122   152.0       2.0       0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "2123   151.0       3.0       1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "2124   152.0       4.0       1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "2125   151.0       4.0       1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "2126   145.0       1.0       0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      SUSP  CLASS  NSP  \n",
       "1      0.0    9.0  2.0  \n",
       "2      0.0    6.0  1.0  \n",
       "3      0.0    6.0  1.0  \n",
       "4      0.0    6.0  1.0  \n",
       "5      0.0    2.0  1.0  \n",
       "...    ...    ...  ...  \n",
       "2122   0.0    5.0  2.0  \n",
       "2123   0.0    5.0  2.0  \n",
       "2124   0.0    5.0  2.0  \n",
       "2125   0.0    5.0  2.0  \n",
       "2126   0.0    1.0  1.0  \n",
       "\n",
       "[2126 rows x 37 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Our Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading Dataset\n",
    "df = pd.read_excel(\"Fetal Monitoring.xls\", sheet_name = \"Raw Data\").dropna()\n",
    "\n",
    "#Dropping Identifiers\n",
    "df = df.drop(columns = \"FileName\")\n",
    "df = df.drop(columns = \"Date\")\n",
    "df = df.drop(columns = \"SegFile\")\n",
    "\n",
    "#Viewing Dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b           float64\n",
       "e           float64\n",
       "LBE         float64\n",
       "LB          float64\n",
       "AC          float64\n",
       "FM          float64\n",
       "UC          float64\n",
       "ASTV        float64\n",
       "MSTV        float64\n",
       "ALTV        float64\n",
       "MLTV        float64\n",
       "DL          float64\n",
       "DS          float64\n",
       "DP          float64\n",
       "DR          float64\n",
       "Width       float64\n",
       "Min         float64\n",
       "Max         float64\n",
       "Nmax        float64\n",
       "Nzeros      float64\n",
       "Mode        float64\n",
       "Mean        float64\n",
       "Median      float64\n",
       "Variance    float64\n",
       "Tendency    float64\n",
       "A           float64\n",
       "B           float64\n",
       "C           float64\n",
       "D           float64\n",
       "E           float64\n",
       "AD          float64\n",
       "DE          float64\n",
       "LD          float64\n",
       "FS          float64\n",
       "SUSP        float64\n",
       "CLASS       float64\n",
       "NSP         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b            float64\n",
       "e            float64\n",
       "LBE          float64\n",
       "LB           float64\n",
       "AC           float64\n",
       "FM           float64\n",
       "UC           float64\n",
       "ASTV         float64\n",
       "MSTV         float64\n",
       "ALTV         float64\n",
       "MLTV         float64\n",
       "DL           float64\n",
       "DS           float64\n",
       "DP           float64\n",
       "DR           float64\n",
       "Width        float64\n",
       "Min          float64\n",
       "Max          float64\n",
       "Nmax         float64\n",
       "Nzeros       float64\n",
       "Mode         float64\n",
       "Mean         float64\n",
       "Median       float64\n",
       "Variance     float64\n",
       "Tendency    category\n",
       "A           category\n",
       "B           category\n",
       "C           category\n",
       "D           category\n",
       "E           category\n",
       "AD          category\n",
       "DE          category\n",
       "LD          category\n",
       "FS           float64\n",
       "SUSP        category\n",
       "CLASS       category\n",
       "NSP         category\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-coding Categorical Variables\n",
    "df['NSP'] = df['NSP'].astype('category')\n",
    "df['Tendency'] = df['Tendency'].astype('category')\n",
    "df['A'] = df['A'].astype('category')\n",
    "df['B'] = df['B'].astype('category')\n",
    "df['C'] = df['C'].astype('category')\n",
    "df['D'] = df['D'].astype('category')\n",
    "df['E'] = df['E'].astype('category')\n",
    "df['AD'] = df['AD'].astype('category')\n",
    "df['DE'] = df['DE'].astype('category')\n",
    "df['LD'] = df['LD'].astype('category')\n",
    "df['SUSP'] = df['SUSP'].astype('category')\n",
    "df['CLASS'] = df['CLASS'].astype('category')\n",
    "\n",
    "#Confirming were encoded correctly\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description of Variables__\n",
    "\n",
    "FileName and SegFile = Personal Identifiers\n",
    "\n",
    "Date = Individual Date of Measurement\n",
    "\n",
    "b = Starting Point of Measurement\n",
    "\n",
    "e = Ending Point of Measurement\n",
    "\n",
    "LBE = Baseline value (By Medical Expert)\n",
    "\n",
    "LB = Fetal Heart Rate Baseline (beats/minute - Automated)\n",
    "\n",
    "AC = # Accelerations/second\n",
    "\n",
    "FM = # Fetal Movements/second\n",
    "\n",
    "UC = # Uterine Contractions/second\n",
    "\n",
    "DL = # Light Decelerations/second\n",
    "\n",
    "DS = # Severe Decelerations/second\n",
    "\n",
    "DP = # Prolonged Decelerations/second\n",
    "\n",
    "DR = # Repetitive Decelerations (All 0s)\n",
    "\n",
    "ASTV = % of time with abnormal short term variability\n",
    "\n",
    "MSTV = Mean Value of Short Term Variability\n",
    "\n",
    "ALTV = % of time with abnormal long term variability\n",
    "\n",
    "MLTV = Mean Value of Long Term Variability\n",
    "\n",
    "Width = Width of Fetal Heart Rate Histogram\n",
    "\n",
    "Min = Minimum of Fetal Heart Rate Histogram\n",
    "\n",
    "Max = Maximum of Fetal Heart Rate Histogram\n",
    "\n",
    "Nmax = # Histogram Peaks\n",
    "\n",
    "Nzeros = # Histogram Zeros\n",
    "\n",
    "Mode = Histogram Mode\n",
    "\n",
    "Mean = Histogram Mean\n",
    "\n",
    "Median = Histogram Median\n",
    "\n",
    "Variance = Histogram Variance\n",
    "\n",
    "Tendency = Histogram Tendency:  -1 = Left Asymmetric, 0 = Symmetric, 1 = Right Asymmetric\n",
    "\n",
    "A = Calm Sleep\n",
    "\n",
    "B = REM Sleep\n",
    "\n",
    "C = Calm Vigilance\n",
    "\n",
    "D = Active Vigilance\n",
    "\n",
    "AD = Accelerative/Decelerative Pattern (Stress Situation)\n",
    "\n",
    "DE = Decelerative Pattern (Vagal Stimulation)\n",
    "\n",
    "LD = Largely decelerative pattern\n",
    "\n",
    "FS = Flat Sinusoidal Pattern (Pathologic State)\n",
    "\n",
    "SUSP = Suspect Pattern\n",
    "\n",
    "Class = 1 to 10 for Classes A to SUSP\n",
    "\n",
    "NSP = Fetal State Class Code (Normal = 1, Suspect = 2, Pathologic = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In the R portion of this project, high levels of correlation were found between the b and e variables, the expert and automated determination of fetal heart rate, and finally mode, mean, and median variables were all highly correlated with one another.__\n",
    "\n",
    "__In this dataset, there is a 10 label multi-class target as well as the 3 label multi-class target variable.  I will be using the 3 label multi-class target variable for my target.__  \n",
    "\n",
    "__The target variables is NSP which classifies the FHR as Normal, Suspect, or Pathologic.__\n",
    "\n",
    "__However, I will first include the 10 label target variables as predictors to see if this increases model performance after hyperparameter tuning and then I will do this without the 10 class variables to see if the performance is better or worse.__\n",
    "\n",
    "__I plan on checking K Nearest Neighbors, Decision Tree, and Random Forest algorithms to determine the best model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'LB', 'Median']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Features for High Correlations With Correlation >\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "#Selecting Upper Triangle of Correlation Matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                                 k=1).astype(np.bool))\n",
    "\n",
    "#Find index of feature columns with correlation >0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >0.90)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Columns with High Correlations >0.90\n",
    "#I will actually drop LBE since this is the expert determined variable.  We want to test the applicability of the SIS Porto system so we will\n",
    "#keep the LB (automated) one.\n",
    "\n",
    "#Dropping 'LB' from our Dataframe\n",
    "df = df.drop(columns = 'LBE')\n",
    "df = df.drop(columns = 'e')\n",
    "df = df.drop(columns = 'Median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>MLTV</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>DR</th>\n",
       "      <th>Width</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>411.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>533.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>2059.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>2796.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2126 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           b     LB   AC   FM   UC  ASTV  MSTV  ALTV  MLTV   DL   DS   DP  \\\n",
       "1      240.0  120.0  0.0  0.0  0.0  73.0   0.5  43.0   2.4  0.0  0.0  0.0   \n",
       "2        5.0  132.0  4.0  0.0  4.0  17.0   2.1   0.0  10.4  2.0  0.0  0.0   \n",
       "3      177.0  133.0  2.0  0.0  5.0  16.0   2.1   0.0  13.4  2.0  0.0  0.0   \n",
       "4      411.0  134.0  2.0  0.0  6.0  16.0   2.4   0.0  23.0  2.0  0.0  0.0   \n",
       "5      533.0  132.0  4.0  0.0  5.0  16.0   2.4   0.0  19.9  0.0  0.0  0.0   \n",
       "...      ...    ...  ...  ...  ...   ...   ...   ...   ...  ...  ...  ...   \n",
       "2122  2059.0  140.0  0.0  0.0  6.0  79.0   0.2  25.0   7.2  0.0  0.0  0.0   \n",
       "2123  1576.0  140.0  1.0  0.0  9.0  78.0   0.4  22.0   7.1  0.0  0.0  0.0   \n",
       "2124  1576.0  140.0  1.0  0.0  7.0  79.0   0.4  20.0   6.1  0.0  0.0  0.0   \n",
       "2125  1576.0  140.0  1.0  0.0  9.0  78.0   0.4  27.0   7.0  0.0  0.0  0.0   \n",
       "2126  2796.0  142.0  1.0  1.0  5.0  74.0   0.4  36.0   5.0  0.0  0.0  0.0   \n",
       "\n",
       "       DR  Width    Min    Max  Nmax  Nzeros   Mode   Mean  Variance Tendency  \\\n",
       "1     0.0   64.0   62.0  126.0   2.0     0.0  120.0  137.0      73.0      1.0   \n",
       "2     0.0  130.0   68.0  198.0   6.0     1.0  141.0  136.0      12.0      0.0   \n",
       "3     0.0  130.0   68.0  198.0   5.0     1.0  141.0  135.0      13.0      0.0   \n",
       "4     0.0  117.0   53.0  170.0  11.0     0.0  137.0  134.0      13.0      1.0   \n",
       "5     0.0  117.0   53.0  170.0   9.0     0.0  137.0  136.0      11.0      1.0   \n",
       "...   ...    ...    ...    ...   ...     ...    ...    ...       ...      ...   \n",
       "2122  0.0   40.0  137.0  177.0   4.0     0.0  153.0  150.0       2.0      0.0   \n",
       "2123  0.0   66.0  103.0  169.0   6.0     0.0  152.0  148.0       3.0      1.0   \n",
       "2124  0.0   67.0  103.0  170.0   5.0     0.0  153.0  148.0       4.0      1.0   \n",
       "2125  0.0   66.0  103.0  169.0   6.0     0.0  152.0  147.0       4.0      1.0   \n",
       "2126  0.0   42.0  117.0  159.0   2.0     1.0  145.0  143.0       1.0      0.0   \n",
       "\n",
       "        A    B    C    D    E   AD   DE   LD   FS SUSP CLASS  NSP  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   9.0  2.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   6.0  1.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   6.0  1.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   6.0  1.0  \n",
       "5     0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   2.0  1.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...  \n",
       "2122  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   5.0  2.0  \n",
       "2123  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   5.0  2.0  \n",
       "2124  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   5.0  2.0  \n",
       "2125  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   5.0  2.0  \n",
       "2126  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  1.0  \n",
       "\n",
       "[2126 rows x 34 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SupervisedIntegerMixin.fit of KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=3, p=2,\n",
      "                     weights='uniform')>\n",
      "[[326   0   0]\n",
      " [  1  67   0]\n",
      " [  0   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       326\n",
      "           1       0.99      0.99      0.99        68\n",
      "           2       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           1.00       426\n",
      "   macro avg       0.99      0.98      0.99       426\n",
      "weighted avg       1.00      1.00      1.00       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting Up Features and Target Variables\n",
    "target = df['NSP']\n",
    "features = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target = pd.get_dummies(target)\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "#Importing Packages for KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Create Standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Standardize Features\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "#Train/Test 80/20 Split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_standardized, target, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating Classifier with K of 3\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, n_jobs = -1)\n",
    "\n",
    "#Fitting Classifier on Trianing Data\n",
    "knn.fit(features_train, target_train)\n",
    "print(knn.fit)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "target_pred = knn.predict(features_test)\n",
    "test0 = np.array(target_test).argmax(axis = 1)\n",
    "predictions0 = np.array(target_pred).argmax(axis = 1)\n",
    "print(confusion_matrix(test0, predictions0))\n",
    "\n",
    "#Printing Classification Report for KNN with N-Neighbors of 3\n",
    "print(classification_report(test0, predictions0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Leaf Size: 1\n",
      "Best p: 1\n",
      "Best n_neighbors: 3\n",
      "Best Metric: minkowski\n",
      "Best Weights: uniform\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning for KNN Using Grid Search CV\n",
    "\n",
    "#Creating Hyperparameter Grid\n",
    "param_dist1 = {\"leaf_size\": list(range(1,50)),\n",
    "              \"n_neighbors\": list(range(1,30)),\n",
    "              \"p\": [1,2]}\n",
    "\n",
    "#Create New KNN Object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use Gridsearch\n",
    "clf = GridSearchCV(knn_2, param_dist1, cv=10, n_jobs=-1)\n",
    "\n",
    "#Fit Model\n",
    "best_model = clf.fit(features_standardized, target)\n",
    "\n",
    "print('Best Leaf Size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Metric:', best_model.best_estimator_.get_params()['metric'])\n",
    "print('Best Weights:', best_model.best_estimator_.get_params()['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[326   0   0]\n",
      " [  2  66   0]\n",
      " [  0   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       326\n",
      "           1       0.99      0.97      0.98        68\n",
      "           2       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.99       426\n",
      "   macro avg       0.99      0.98      0.99       426\n",
      "weighted avg       0.99      0.99      0.99       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking Best KNN Model Using Leaf Size of 1, P of 1, and Best Neighbors of 3, Uniform Weights\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target = df['NSP']\n",
    "features = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target = pd.get_dummies(target)\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "#Create Standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Standardize Features\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "#Train/Test 80/20 Split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_standardized, target, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating Classifier with K of 3\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, n_jobs = -1, p = 1, leaf_size = 1, weights = \"uniform\", metric = \"minkowski\")\n",
    "\n",
    "#Fitting Classifier on Trianing Data\n",
    "knn.fit(features_train, target_train)\n",
    "\n",
    "#Printing Classification Report\n",
    "target_pred = knn.predict(features_test)\n",
    "test0 = np.array(target_test).argmax(axis = 1)\n",
    "predictions0 = np.array(target_pred).argmax(axis = 1)\n",
    "print(confusion_matrix(test0, predictions0))\n",
    "\n",
    "#Printing Classification Report\n",
    "print(classification_report(test0, predictions0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "[[324   2   0]\n",
      " [  1  67   0]\n",
      " [  0   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.99      1.00       326\n",
      "         2.0       0.96      0.99      0.97        68\n",
      "         3.0       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.99       426\n",
      "   macro avg       0.98      0.98      0.98       426\n",
      "weighted avg       0.99      0.99      0.99       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting Data On Decision Tree Algorithm\n",
    "\n",
    "#Importing Packages\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target = df['NSP']\n",
    "features = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train1, features_test1, target_train1, target_test1 = train_test_split(features, target, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Instantiating Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state = 1)\n",
    "print(dt.fit(features_train1, target_train1))\n",
    "\n",
    "#Setting Target Prediction Variable Based on Features Test\n",
    "target_pred = dt.predict(features_test1)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test1 = np.array(target_test1)\n",
    "predictions1 = np.array(target_pred)\n",
    "print(confusion_matrix(test1, predictions1))\n",
    "\n",
    "#Classification Report for Decision Tree\n",
    "print(classification_report(test1, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blmay\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "Best Score is 0.9847058823529412\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning for Decision Tree\n",
    "\n",
    "#Creating Hyperparameter Grid\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "             \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "tree_cv = RandomizedSearchCV(dt, param_dist, cv = 10)\n",
    "\n",
    "tree_cv.fit(features_train1, target_train1)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_estimator_))\n",
    "print(\"Best Score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[321   5   0]\n",
      " [  2  66   0]\n",
      " [  0   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.99      0.98      0.99       326\n",
      "         2.0       0.92      0.97      0.94        68\n",
      "         3.0       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.98       426\n",
      "   macro avg       0.97      0.97      0.97       426\n",
      "weighted avg       0.98      0.98      0.98       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting New DT Algorithm on Tuned Hyperparameters\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target = df['NSP']\n",
    "features = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train1, features_test1, target_train1, target_test1 = train_test_split(features, target, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Instantiating Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
    "                       max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort=False,\n",
    "                       random_state=1, splitter='best')\n",
    "dt.fit(features_train1, target_train1)\n",
    "\n",
    "#Setting Target Prediction Variable Based on Features Test\n",
    "target_pred = dt.predict(features_test1)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test1 = np.array(target_test1)\n",
    "predictions1 = np.array(target_pred)\n",
    "print(confusion_matrix(test1, predictions1))\n",
    "\n",
    "#Classification Report for Decision Tree\n",
    "print(classification_report(test1, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "[[326   0   0]\n",
      " [  2  66   0]\n",
      " [  0   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.99      1.00      1.00       326\n",
      "         2.0       0.99      0.97      0.98        68\n",
      "         3.0       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.99       426\n",
      "   macro avg       0.99      0.98      0.99       426\n",
      "weighted avg       0.99      0.99      0.99       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting Data on Random Forest Algorithm\n",
    "\n",
    "#Importing Packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target = df['NSP']\n",
    "features = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train2, features_test2, target_train2, target_test2 = train_test_split(features, target, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating RFC Model\n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 1)\n",
    "\n",
    "#Fitting Training Data\n",
    "print(model.fit(features_train2, target_train2))\n",
    "\n",
    "#Checking Predictions\n",
    "rf_predictions = model.predict(features_test2)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test2 = np.array(target_test2)\n",
    "predictions2 = np.array(rf_predictions)\n",
    "print(confusion_matrix(test2, predictions2))\n",
    "\n",
    "#Classification Report\n",
    "print(classification_report(test2, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_s...\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 311, 522, 733,\n",
       "                                                         944, 1155, 1366, 1577,\n",
       "                                                         1788, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameter Tuning for Random Forest Algorithm\n",
    "\n",
    "#Number of tress in Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "\n",
    "#Number of Features to Consider At Each Split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "#Maximum Number of Levels in Tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "#Minimum Number of Samples Required At Each Leaf\n",
    "min_samples_split = [2,5,10]\n",
    "\n",
    "#Minimum Number of Samles Required at Each Leaf Node\n",
    "min_samples_leaf = [1,2,4]\n",
    "\n",
    "#Method of Selecting Samples for Training Each Tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "#Create Random Grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "#Using Random Grid to Search for Best Hyperparameters\n",
    "\n",
    "rf_random1 = RandomForestClassifier()\n",
    "\n",
    "rf_random2 = RandomizedSearchCV(estimator = rf_random1, param_distributions = random_grid, n_iter = 100, cv = 10, verbose = 2, random_state = 1, n_jobs = -1)\n",
    "\n",
    "#Fitting the Model\n",
    "rf_random2.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[326   0   0]\n",
      " [  2  66   0]\n",
      " [  0   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.99      1.00      1.00       326\n",
      "         2.0       0.99      0.97      0.98        68\n",
      "         3.0       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.99       426\n",
      "   macro avg       0.99      0.98      0.99       426\n",
      "weighted avg       0.99      0.99      0.99       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Running New RFC with New Hyperparameters\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target = df['NSP']\n",
    "features = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train2, features_test2, target_train2, target_test2 = train_test_split(features, target, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating RFC Model\n",
    "model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "#Fitting Training Data\n",
    "model.fit(features_train2, target_train2)\n",
    "\n",
    "#Checking Predictions\n",
    "rf_predictions = model.predict(features_test2)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test2 = np.array(target_test2)\n",
    "predictions2 = np.array(rf_predictions)\n",
    "print(confusion_matrix(test2, predictions2))\n",
    "\n",
    "#Classification Report\n",
    "print(classification_report(test2, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running KNN With Our Other Target Variables Eliminated A, B, C, D, E, AD, DE, LD, FS, SUSP, CLASS\n",
    "df = df.drop(columns = ['A', 'B', 'C', 'D', 'E', 'AD', 'DE', 'LD', 'FS', 'SUSP', 'CLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[319   6   1]\n",
      " [ 23  40   5]\n",
      " [  7   6  19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       326\n",
      "           1       0.77      0.59      0.67        68\n",
      "           2       0.76      0.59      0.67        32\n",
      "\n",
      "    accuracy                           0.89       426\n",
      "   macro avg       0.81      0.72      0.76       426\n",
      "weighted avg       0.88      0.89      0.88       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting New Features and Target and Checking KNN\n",
    "target_new = df['NSP']\n",
    "features_new = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target_new = pd.get_dummies(target_new)\n",
    "features_new = pd.get_dummies(features_new)\n",
    "\n",
    "#Create Standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Standardize Features\n",
    "features_standardized = standardizer.fit_transform(features_new)\n",
    "\n",
    "#Train/Test 80/20 Split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_standardized, target_new, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, leaf_size = 1, p = 1, n_jobs = -1)\n",
    "\n",
    "#Fitting Classifier on Trianing Data\n",
    "knn.fit(features_train, target_train)\n",
    "\n",
    "target_pred = knn.predict(features_test)\n",
    "test0 = np.array(target_test).argmax(axis = 1)\n",
    "predictions0 = np.array(target_pred).argmax(axis = 1)\n",
    "print(confusion_matrix(test0, predictions0))\n",
    "\n",
    "#Printing Classification Report for KNN\n",
    "print(classification_report(test0, predictions0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Leaf Size: 1\n",
      "Best p: 2\n",
      "Best n_neighbors: 20\n",
      "Best Metric: minkowski\n",
      "Best Weights: uniform\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning for KNN Using Grid Search CV With the 10 Predictors Removed\n",
    "\n",
    "#Creating Hyperparameter Grid\n",
    "param_dist1 = {\"leaf_size\": list(range(1,50)),\n",
    "              \"n_neighbors\": list(range(1,30)),\n",
    "              \"p\": [1,2]}\n",
    "\n",
    "#Create New KNN Object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use Gridsearch\n",
    "clf = GridSearchCV(knn_2, param_dist1, cv=10, n_jobs=-1)\n",
    "\n",
    "#Fit Model\n",
    "best_model = clf.fit(features_standardized, target)\n",
    "\n",
    "print('Best Leaf Size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Metric:', best_model.best_estimator_.get_params()['metric'])\n",
    "print('Best Weights:', best_model.best_estimator_.get_params()['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[321   5   0]\n",
      " [ 32  35   1]\n",
      " [ 10   6  16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       326\n",
      "           1       0.76      0.51      0.61        68\n",
      "           2       0.94      0.50      0.65        32\n",
      "\n",
      "    accuracy                           0.87       426\n",
      "   macro avg       0.86      0.67      0.73       426\n",
      "weighted avg       0.87      0.87      0.86       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Doing KNN Using Hyperparameters Found Above\n",
    "\n",
    "#Setting New Features and Target and Checking KNN\n",
    "target_new = df['NSP']\n",
    "features_new = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target_new = pd.get_dummies(target_new)\n",
    "features_new = pd.get_dummies(features_new)\n",
    "\n",
    "#Create Standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Standardize Features\n",
    "features_standardized = standardizer.fit_transform(features_new)\n",
    "\n",
    "#Train/Test 80/20 Split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_standardized, target_new, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 20, leaf_size = 1, p = 2, metric = \"minkowski\", weights = \"uniform\", n_jobs = -1)\n",
    "\n",
    "#Fitting Classifier on Trianing Data\n",
    "knn.fit(features_train, target_train)\n",
    "\n",
    "target_pred = knn.predict(features_test)\n",
    "test0 = np.array(target_test).argmax(axis = 1)\n",
    "predictions0 = np.array(target_pred).argmax(axis = 1)\n",
    "print(confusion_matrix(test0, predictions0))\n",
    "\n",
    "#Printing Classification Report for KNN\n",
    "print(classification_report(test0, predictions0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Based on the results here, the model actually performed worse by the F1-score, even when using hyperparameter tuning, when removing the other 10-class predictors to be used as prediction for our target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308  17   1]\n",
      " [ 16  46   6]\n",
      " [  1   2  29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.94      0.95       326\n",
      "         2.0       0.71      0.68      0.69        68\n",
      "         3.0       0.81      0.91      0.85        32\n",
      "\n",
      "    accuracy                           0.90       426\n",
      "   macro avg       0.82      0.84      0.83       426\n",
      "weighted avg       0.90      0.90      0.90       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting Data On Decision Tree Algorithm\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target_new_1 = df['NSP']\n",
    "features_new_1 = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target_new = pd.get_dummies(target_new_1)\n",
    "features_new = pd.get_dummies(features_new_1)\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train11, features_test11, target_train11, target_test11 = train_test_split(features_new_1, target_new_1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Instantiating Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(features_train11, target_train11)\n",
    "\n",
    "#Setting Target Prediction Variable Based on Features Test\n",
    "target_pred1 = dt.predict(features_test11)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test11 = np.array(target_test11)\n",
    "predictions11 = np.array(target_pred1)\n",
    "print(confusion_matrix(test11, predictions11))\n",
    "\n",
    "#Classification Report for Decision Tree\n",
    "print(classification_report(test11, predictions11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blmay\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Best Score is 0.9335294117647058\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning Using New Dataset\n",
    "\n",
    "#Creating Hyperparameter Grid\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "             \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "tree_cv = RandomizedSearchCV(dt, param_dist, cv = 10)\n",
    "\n",
    "tree_cv.fit(features_train11, target_train11)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_estimator_))\n",
    "print(\"Best Score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[314  10   2]\n",
      " [ 16  47   5]\n",
      " [  1   3  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.96      0.96       326\n",
      "         2.0       0.78      0.69      0.73        68\n",
      "         3.0       0.80      0.88      0.84        32\n",
      "\n",
      "    accuracy                           0.91       426\n",
      "   macro avg       0.84      0.84      0.84       426\n",
      "weighted avg       0.91      0.91      0.91       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting Data On Decision Tree Algorithm With Hyperparameters Specified\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target_new_1 = df['NSP']\n",
    "features_new_1 = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target_new = pd.get_dummies(target_new_1)\n",
    "features_new = pd.get_dummies(features_new_1)\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train11, features_test11, target_train11, target_test11 = train_test_split(features_new_1, target_new_1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Instantiating Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
    "                       max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort=False,\n",
    "                       random_state=None, splitter='best')\n",
    "dt.fit(features_train11, target_train11)\n",
    "\n",
    "#Setting Target Prediction Variable Based on Features Test\n",
    "target_pred1 = dt.predict(features_test11)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test11 = np.array(target_test11)\n",
    "predictions11 = np.array(target_pred1)\n",
    "print(confusion_matrix(test11, predictions11))\n",
    "\n",
    "#Classification Report for Decision Tree\n",
    "print(classification_report(test11, predictions11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Based on removing the 10 point classifier as predictors, the Decision Tree did perform worse though with the hyperparameter tuning that was used before, the model had fairly positive results except for the Suspect class with a lower F1 score as compared to Normal and Pathologic categories__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[325   0   1]\n",
      " [ 20  45   3]\n",
      " [  4   6  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       326\n",
      "           1       0.88      0.66      0.76        68\n",
      "           2       0.85      0.69      0.76        32\n",
      "\n",
      "   micro avg       0.94      0.92      0.93       426\n",
      "   macro avg       0.89      0.78      0.83       426\n",
      "weighted avg       0.94      0.92      0.92       426\n",
      " samples avg       0.92      0.92      0.92       426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blmay\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\blmay\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Running Random Forest Algorithm on New Dataset\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target_new_2 = df['NSP']\n",
    "features_new_2 = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target_new_2 = pd.get_dummies(target_new_2)\n",
    "features_new_2 = pd.get_dummies(features_new_2)\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train22, features_test22, target_train22, target_test22 = train_test_split(features_new_2, target_new_2, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating RFC Model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Fitting Training Data\n",
    "rf.fit(features_train22, target_train22)\n",
    "\n",
    "#Checking Predictions\n",
    "rf_predictions1 = rf.predict(features_test22)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test22 = np.array(target_test22)\n",
    "predictions22 = np.array(rf_predictions1)\n",
    "print(confusion_matrix(test22.argmax(axis=1), predictions22.argmax(axis=1)))\n",
    "\n",
    "#Classification Report\n",
    "print(classification_report(test22, predictions22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_s...\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 311, 522, 733,\n",
       "                                                         944, 1155, 1366, 1577,\n",
       "                                                         1788, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameter Tuning for Random Forest Algorithm with the New Dataset\n",
    "\n",
    "#Number of tress in Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "\n",
    "#Number of Features to Consider At Each Split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "#Maximum Number of Levels in Tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "#Minimum Number of Samples Required At Each Leaf\n",
    "min_samples_split = [2,5,10]\n",
    "\n",
    "#Minimum Number of Samles Required at Each Leaf Node\n",
    "min_samples_leaf = [1,2,4]\n",
    "\n",
    "#Method of Selecting Samples for Training Each Tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "#Create Random Grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "#Using Random Grid to Search for Best Hyperparameters\n",
    "\n",
    "rf_random1 = RandomForestClassifier()\n",
    "\n",
    "rf_random2 = RandomizedSearchCV(estimator = rf_random1, param_distributions = random_grid, n_iter = 100, cv = 10, verbose = 2, random_state = 1, n_jobs = -1)\n",
    "\n",
    "#Fitting the Model\n",
    "rf_random2.fit(features_train22, target_train22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[324   2   0]\n",
      " [ 23  42   3]\n",
      " [  5   1  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       326\n",
      "           1       0.93      0.62      0.74        68\n",
      "           2       0.90      0.81      0.85        32\n",
      "\n",
      "   micro avg       0.94      0.92      0.93       426\n",
      "   macro avg       0.92      0.81      0.85       426\n",
      "weighted avg       0.94      0.92      0.92       426\n",
      " samples avg       0.92      0.92      0.92       426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blmay\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Running Random Forest Algorithm on New Dataset with Tuned Hyperparameters\n",
    "\n",
    "#Setting Up Features and Target Variables\n",
    "target_new_2 = df['NSP']\n",
    "features_new_2 = df.loc[:, df.columns != 'NSP']\n",
    "\n",
    "#Getting Dummy Variables for our categorical variables\n",
    "target_new_2 = pd.get_dummies(target_new_2)\n",
    "features_new_2 = pd.get_dummies(features_new_2)\n",
    "\n",
    "#Train/Test Splitting with 80/20 Split\n",
    "features_train22, features_test22, target_train22, target_test22 = train_test_split(features_new_2, target_new_2, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Creating RFC Model\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                                                    criterion='gini',\n",
    "                                                    max_depth=None,\n",
    "                                                    max_features='auto',\n",
    "                                                    max_leaf_nodes=None,\n",
    "                                                    min_impurity_decrease=0.0,\n",
    "                                                    min_impurity_split=None,\n",
    "                                                    min_samples_leaf=1,\n",
    "                                                    min_samples_split=2,\n",
    "                                                    min_weight_fraction_leaf=0.0,\n",
    "                                                    n_estimators=100,\n",
    "                                                    n_jobs = -1)\n",
    "#Fitting Training Data\n",
    "rf.fit(features_train22, target_train22)\n",
    "\n",
    "#Checking Predictions\n",
    "rf_predictions1 = rf.predict(features_test22)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "test22 = np.array(target_test22)\n",
    "predictions22 = np.array(rf_predictions1)\n",
    "print(confusion_matrix(test22.argmax(axis=1), predictions22.argmax(axis=1)))\n",
    "\n",
    "#Classification Report\n",
    "print(classification_report(test22, predictions22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In this case as well, the Random Forest model did perform worse without the 10 classifier system that was in the model before.__\n",
    "\n",
    "__Using the results above, both the Random Forest and KNN models performed similarly after hyperparameter tuning and removing the extraneous variables of e, LBE, and Median.  Further the 10 point classification variables used as predictors actually improved the F1, Precision, and Recall scores so those variables should be kept in the model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
